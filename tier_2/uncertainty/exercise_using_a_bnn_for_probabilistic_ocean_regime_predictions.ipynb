{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxHHIgGoAVqW"
   },
   "source": [
    "# Using a BNN for probabilistic ocean regime predictions\n",
    "### EXERCISE\n",
    "\n",
    "The exercise below is based on the work done in \n",
    "\n",
    "Clare et al. (2022) *Explainable Artificial Intelligence for Bayesian Neural Networks: Toward Trustworthy Predictions of Ocean Dynamics* Journal of Advances in Modeling Earth Systems 14 (11), e2022MS003162 [link](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2022MS003162)\n",
    "\n",
    "In that paper, a Bayesian Neural Network was used to make a probabilistic prediction of ocean circulation regimes. You will learn more about this dataset and how the ocean regimes were identified from Maike Sonnewald in Tier 3. Note that Clare et al. (2022) builds on work in [Sonnewald & Lguensat (2021)](https://doi.org/10.1029/2021MS002496) who built a deterministic neural network for the same problem and [Sonnewald et al. (2019)](https://doi.org/10.1029/2018ea000519), where the ocean regimes were first identified by k-means clustering. \n",
    "\n",
    "The input features for our neural network are as follows:\n",
    "\n",
    "1.   Wind stress curl\n",
    "2.   Mean Sea Surface Height (SSH) (20 years)\n",
    "3.   Gradients of Mean Sea Surface Height\n",
    "4.   Bathymetry\n",
    "5.   Gradients of bathymetry\n",
    "6.   Coriolis\n",
    "\n",
    "For justification of this choice of these features please see Sonnewald & Lguensat (2021). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run this exercise\n",
    "\n",
    "This exercise is in the form of a [Jupyter notebook](https://jupyter.org/). It can be run in a number of free cloud based environments (see two options below). These require no installation. When you click on one of the links below (\"Open in Colab\" or \"Open in Kaggle\") you will be prompted to create a free account, after which you will see the same page you see here. Follow the instructions below to connect to a GPU. After that you can run each block of code by selecting shift+control repeatedly, or by selecting the \"play\" icon.\n",
    "\n",
    "Please note that in this exercise you will be asked to insert code. However, those less familiar with Python can refer to the solutions notebook, which requires no coding.\n",
    "\n",
    "Advanced users may wish to run this exercise on their own computers by first installing Python and Jupyter, in addition to the packages listed below (numpy, xarray, scipy and matplotlib)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "td, th {\n",
    "   border: 1px solid white;\n",
    "   border-collapse: collapse;\n",
    "}\n",
    "</style>\n",
    "<table align=\"left\">\n",
    "  <tr>\n",
    "    <th>Run the tutorial via free cloud platforms: </th>\n",
    "    <th><a href=\"https://colab.research.google.com/github/ecmwf-projects/mooc-machine-learning-weather-climate/blob/main/tier_2/uncertainty/exercise_using_a_bnn_for_probabilistic_ocean_regime_predictions.ipynb\">\n",
    "        <img src = \"https://colab.research.google.com/assets/colab-badge.svg\" alt = \"Colab\"></th>\n",
    "    <th><a href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ecmwf-projects/mooc-machine-learning-weather-climate/blob/main/tier_2/uncertainty/exercise_using_a_bnn_for_probabilistic_ocean_regime_predictions.ipynb\">\n",
    "        <img src = \"https://kaggle.com/static/images/open-in-kaggle.svg\" alt = \"Kaggle\"></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDDNQgp796BP"
   },
   "source": [
    "Note that to use Kaggle, you need to enable an option on the notebook. Please follow the instructions here to do this https://stackoverflow.com/questions/68142524/cannot-access-internet-on-kaggle-notebook. \n",
    "\n",
    "Since we will train a neural network later, it might make sense to connect to a GPU runtime:\n",
    "\n",
    "*   In Google Colab this can be done under Runtime --> Change runtime type.\n",
    "\n",
    "*   In Kaggle this can be done under Accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B42veS4ddyYc"
   },
   "outputs": [],
   "source": [
    "## Import standard packages for reading data and plotting\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rx4QDnxwyGPi"
   },
   "source": [
    "## Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmlP72BPqLQL"
   },
   "outputs": [],
   "source": [
    "## Download input Data\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/kCluster6.npy\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/curlTau.npy\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.1992.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.1993.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.1994.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.1995.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.1996.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.1997.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.1998.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.1999.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.2000.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.2001.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.2002.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.2003.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.2004.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.2005.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.2006.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.2007.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.2008.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.2009.nc\n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.2010.nc \n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/SSHdata/SSH.2011.nc    \n",
    "! wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_2/uncertainty/H_wHFacC.mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_stress_curl = np.transpose(np.load('curlTau.npy'))\n",
    "monthly_ssh = xr.open_mfdataset('SSH.*.nc', combine='by_coords')\n",
    "SSH20mean = monthly_ssh['SSH'].mean(axis=0).values  # 20 years mean of sea surface height\n",
    "bathymetry = np.transpose(loadmat('H_wHFacC.mat')['val'])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsBmpFJeAMge"
   },
   "outputs": [],
   "source": [
    "## Load in ocean regimes labels as target data. These ocean regimes were determined in Sonnewald et al. 2019\n",
    "ecco_label = np.transpose(np.load('kCluster6.npy'))\n",
    "\n",
    "# replace land pixels by NaNs\n",
    "ecco_label[ecco_label==-1] = np.nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rO0mt6-jyGPj"
   },
   "outputs": [],
   "source": [
    "## Calculate the SSH gradients, bathymetry gradients and coriolis\n",
    "\n",
    "lonRoll = np.roll(monthly_ssh['lat'].values, axis=0, shift=-1)\n",
    "Londiff = lonRoll - monthly_ssh['lat'].values  # equivalent to doing x_{i} - x_{i-1}\n",
    "\n",
    "lat = monthly_ssh['lat'].values\n",
    "latDiff=1.111774765625000e+05\n",
    "latY=np.gradient(lat, axis=0)*latDiff\n",
    "lonX=np.abs(np.cos(lat*np.pi/180))*latDiff*Londiff\n",
    "\n",
    "##coriolis \n",
    "Omega=7.2921e-5 # coriolis parameter\n",
    "f = (2*Omega*np.sin(lat*np.pi/180))\n",
    "\n",
    "def grad(d,y,x):\n",
    "    grady=np.gradient(d, axis=0)/y\n",
    "    gradx=np.gradient(d, axis=1)/x\n",
    "    return grady, gradx\n",
    "\n",
    "gradSSH_y, gradSSH_x = grad(SSH20mean,latY,lonX)\n",
    "gradBathm_y, gradBathm_x = grad(bathymetry,latY,lonX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rWGyVc8P-Sw"
   },
   "outputs": [],
   "source": [
    "## Plot data\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "plt.subplot(4,2,1)\n",
    "plt.imshow(np.flipud(wind_stress_curl), cmap='seismic')\n",
    "plt.colorbar(shrink=0.5)\n",
    "plt.clim(-1e-9,1e-9)\n",
    "plt.title('Wind stress curl')\n",
    "\n",
    "plt.subplot(4,2,2)\n",
    "plt.imshow(np.flipud(SSH20mean), cmap='seismic')\n",
    "plt.colorbar(shrink=0.5)\n",
    "plt.title('Mean sea surface height')\n",
    "\n",
    "plt.subplot(4,2,3)\n",
    "plt.imshow(np.flipud(gradSSH_x), cmap='seismic')\n",
    "plt.colorbar(shrink=0.5)\n",
    "plt.title('gradSSH_x')\n",
    "\n",
    "plt.subplot(4,2,4)\n",
    "plt.imshow(np.flipud(gradSSH_y), cmap='seismic')\n",
    "plt.colorbar(shrink=0.5)\n",
    "plt.title('gradSSH_y')\n",
    "\n",
    "plt.subplot(4,2,5)\n",
    "plt.imshow(np.flipud(bathymetry), cmap='seismic')\n",
    "plt.colorbar(shrink=0.5)\n",
    "plt.title('Bathymetry')\n",
    "\n",
    "plt.subplot(4,2,6)\n",
    "plt.imshow(np.flipud(gradBathm_x), cmap='seismic')\n",
    "plt.colorbar(shrink=0.5)\n",
    "plt.title('gradBathm_x')\n",
    "\n",
    "plt.subplot(4,2,7)\n",
    "plt.imshow(np.flipud(gradBathm_y), cmap='seismic')\n",
    "plt.colorbar(shrink=0.5)\n",
    "plt.title('gradBathm_y')\n",
    "\n",
    "plt.subplot(4,2,8)\n",
    "plt.imshow(np.flipud(f), cmap='seismic')\n",
    "plt.colorbar(shrink=0.5)\n",
    "plt.title('Coriolis')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHqVDjQCpuaF"
   },
   "outputs": [],
   "source": [
    "## Mask land pixels and other noisy locations\n",
    "missingdataindex = np.isnan(wind_stress_curl*SSH20mean*gradSSH_x*gradSSH_y*bathymetry*gradBathm_x*gradBathm_y)\n",
    "\n",
    "## Training data is ocean dataset excluding the Atlantic Ocean\n",
    "maskTraining = (~missingdataindex).copy()\n",
    "maskTraining[:,200:400]=False\n",
    "\n",
    "## Test dataset is Atlantic Ocean dataset\n",
    "maskTest = (~missingdataindex).copy()\n",
    "maskTest[:,list(range(200))+list(range(400,720))]=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkFqOxy9lDJ1"
   },
   "outputs": [],
   "source": [
    "## Set up training and test datasets\n",
    "\n",
    "TotalDataset = np.stack((wind_stress_curl[~missingdataindex],\n",
    "                         SSH20mean[~missingdataindex],\n",
    "                         gradSSH_x[~missingdataindex],\n",
    "                         gradSSH_y[~missingdataindex],\n",
    "                         bathymetry[~missingdataindex],\n",
    "                         gradBathm_x[~missingdataindex],\n",
    "                         gradBathm_y[~missingdataindex],\n",
    "                         f[~missingdataindex]),1)\n",
    "\n",
    "TrainDataset = np.stack((wind_stress_curl[maskTraining],\n",
    "                         SSH20mean[maskTraining],\n",
    "                         gradSSH_x[maskTraining],\n",
    "                         gradSSH_y[maskTraining],\n",
    "                         bathymetry[maskTraining],\n",
    "                         gradBathm_x[maskTraining],\n",
    "                         gradBathm_y[maskTraining],\n",
    "                         f[maskTraining]),1)\n",
    "\n",
    "TestDataset = np.stack((wind_stress_curl[maskTest],\n",
    "                         SSH20mean[maskTest],\n",
    "                         gradSSH_x[maskTest],\n",
    "                         gradSSH_y[maskTest],\n",
    "                         bathymetry[maskTest],\n",
    "                         gradBathm_x[maskTest],\n",
    "                         gradBathm_y[maskTest],\n",
    "                       f[maskTest]),1)\n",
    "\n",
    "print(TotalDataset.shape, TrainDataset.shape, TestDataset.shape)\n",
    "\n",
    "train_label = ecco_label[maskTraining]\n",
    "test_label = ecco_label[maskTest]\n",
    "print(train_label.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9fEt6N0yGPl"
   },
   "outputs": [],
   "source": [
    "## Scale the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(TrainDataset)\n",
    "scaler.mean_,scaler.scale_\n",
    "\n",
    "X_train_scaled = scaler.transform(TrainDataset)\n",
    "X_test_scaled = scaler.transform(TestDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-VpDOkzuLt2"
   },
   "source": [
    "# Training the Bayesian Neural Network (BNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgsxz9Gtij_d"
   },
   "source": [
    "To use a BNN we must import both `tensorflow` and `tensorflow_probability`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8fM26gtqLQZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# convert target labels to appropriate data type for tensorflow\n",
    "Y_train = tf.keras.utils.to_categorical(train_label)\n",
    "Y_test = tf.keras.utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dh3rpPD-ij_e"
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "The first task is to build a deterministic network to predict the ocean regimes. Evaluate the accuracy of your model.\n",
    "\n",
    "*Hint: The data is shaped for a gridpoint-by-gridpoint approach so Dense layers are appropriate here. The target data is categorical so the last layer should have a SoftMax activation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sac1j7ml1uin"
   },
   "outputs": [],
   "source": [
    "def deterministic_model(model = None):\n",
    "  # Your code here\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the deterministic model\n",
    "det_model = deterministic_model()\n",
    "det_model.summary()\n",
    "det_model.compile(loss = 'categorical_crossentropy', metrics = ['categorical_accuracy'],\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01))\n",
    "det_model.fit(X_train_scaled, Y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split = 0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_G4d95_FMLo"
   },
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of this deterministic model\n",
    "\n",
    "print(det_model.evaluate(X_train_scaled, Y_train))\n",
    "print(det_model.evaluate(X_test_scaled, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wi8dmfG_39Q7"
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "Change your deterministic neural network from Exercise 1 so that it predicts a distribution as an output, quantifying the aleatoric uncertainty. Evaluate the accuracy of your model.\n",
    "\n",
    "\n",
    "*Hint: You should use the OneHotCategorical Layer from Tensorflow probability because this means that the output of the network is a distribution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cOfaWUsb5sJH"
   },
   "outputs": [],
   "source": [
    "def probabilistic_model():\n",
    "    model = None\n",
    "    ### Your code here\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcOT-4wt7HFJ"
   },
   "outputs": [],
   "source": [
    "prob_model = probabilistic_model()\n",
    "\n",
    "prob_model.summary()\n",
    "\n",
    "# define the negative log-likelihood function\n",
    "def nll(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)\n",
    "\n",
    "prob_model.compile(loss=nll,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "prob_model.fit(X_train_scaled, Y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=20,\n",
    "                    verbose=1,\n",
    "                    validation_split = 0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNfO-9mXD4n-"
   },
   "outputs": [],
   "source": [
    "## Example output\n",
    "prob_model(X_test_scaled[0:1]).mean().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2uYB3yVEFG0Z"
   },
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of this first Bayesian model\n",
    "\n",
    "print(prob_model.evaluate(X_train_scaled, Y_train))\n",
    "print(prob_model.evaluate(X_test_scaled, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNk3SEuo7RH4"
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "Change your neural network from Exercise 2 so that it quantifies epistemic uncertainty as well as aleatoric uncertainty. Evaluate the accuracy of your model. The accuracy will almost definitely be less than that from the deterministic model, but with the added benefit that you now have uncertainty information.\n",
    "\n",
    "*Hint: In BNNs, the parameters (weights and biases) are distributions rather than deterministic. Standard practice with tensorflow probability is to determine this distributions using variational inference. Therefore we must define a fixed prior for each of the weights and the general shape for the posterior. Dense layers in a deterministic network are replaced with DenseVariational layers in a BNN. Note that you also have to rescale the KL divergence error so that the arguments for your DenseVariational layer should include the following arguments:*\n",
    "```\n",
    "kl_weight = 1/X_train_scaled.shape[0], # have to rescale the kl_error\n",
    "kl_use_exact=True # use if have analytic form of prior and posterior - may error in which case change to False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FAfLKxj85os"
   },
   "outputs": [],
   "source": [
    "# Define the prior weight distribution -- all N(0, 1) -- and not trainable\n",
    "def prior(kernel_size, bias_size, dtype = None):\n",
    "    n = kernel_size + bias_size\n",
    "    prior_model = Sequential([\n",
    "                            tfpl.DistributionLambda(\n",
    "                                lambda t: tfd.MultivariateNormalDiag(loc = tf.zeros(n), scale_diag = tf.ones(n))\n",
    "                            )\n",
    "    ]) # normal distribution for each weight in the layer\n",
    "    return prior_model\n",
    "\n",
    "# Define variational posterior weight distribution -- multivariate Gaussian\n",
    "\n",
    "def posterior(kernel_size, bias_size, dtype = None):\n",
    "    n = kernel_size + bias_size\n",
    "    posterior_model = Sequential([\n",
    "                tfpl.VariableLayer(2*n, dtype=dtype),\n",
    "                tfpl.DistributionLambda (\n",
    "        lambda t: tfd.MultivariateNormalDiag(loc = t[..., :n], scale_diag = tf.math.exp(t[..., n:])))\n",
    "    ]) # define posterior for each weight in the layer\n",
    "    return posterior_model\n",
    "\n",
    "def bnn():\n",
    "    model = None\n",
    "    ## Your code here\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AqooZjsI9Pv8"
   },
   "outputs": [],
   "source": [
    "bnn_model = bnn()\n",
    "\n",
    "def nll(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)\n",
    "\n",
    "bnn_model.compile(loss=nll,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# add callbacks to save best weights\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'bnn_weights.h5', monitor='val_loss', verbose=1, save_best_only=True,\n",
    "    save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "# and to reduce the learning rate if the error does not improve after 15 epochs\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor = 'val_loss',\n",
    "            patience=15,\n",
    "            factor=0.25,\n",
    "            verbose=1)\n",
    "\n",
    "# fit model\n",
    "bnn_model.fit(X_train_scaled, Y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    validation_split = 0.2, shuffle = True,\n",
    "                    callbacks = [checkpoint_callback, reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JxRCGcyllil"
   },
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of the BNN model\n",
    "\n",
    "print(bnn_model.evaluate(X_train_scaled, Y_train))\n",
    "print(bnn_model.evaluate(X_test_scaled, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ufw8IJwW8fG"
   },
   "source": [
    "## Advanced: Plotting uncertainty metrics obtained from the BNN predictions\n",
    "\n",
    "Below we provide some functions to plot the results of your BNN and assess its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEzlRMyVo_Ko"
   },
   "outputs": [],
   "source": [
    "# Define functions to analyse model predictions versus true labels\n",
    "\n",
    "def boxplot_model_predictions(prob_predictions, point_num, labels, run_ensemble = True):\n",
    "    # Print the true activity\n",
    "    print('------------------------------')\n",
    "    print('True cluster:', labels[point_num])\n",
    "    print('')\n",
    "\n",
    "    # Print the probabilities the model assigns\n",
    "    print('------------------------------')\n",
    "    print('Model estimated probabilities:')\n",
    "    # Create ensemble of predicted probabilities\n",
    "\n",
    "    predicted_probabilities = prob_predictions[:, point_num, :]\n",
    "    box = plt.boxplot(predicted_probabilities, positions = [0, 1, 2, 3, 4, 5])\n",
    "    for i in range(6):\n",
    "        if i == int(labels[point_num]):\n",
    "            plt.setp(box['boxes'][i], color='green')\n",
    "            plt.setp(box['medians'][i], color='green')\n",
    "        else:\n",
    "            plt.setp(box['boxes'][i], color='purple')\n",
    "            plt.setp(box['medians'][i], color='purple')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xticks([0, 1, 2, 3, 4, 5], [\"F\", \"E\", \"D\", \"C\", \"B\", \"A\"])\n",
    "    plt.xlim([5.5, -0.5])\n",
    "    plt.show()        \n",
    "    return predicted_probabilities\n",
    "    \n",
    "def get_correct_indices(prob_mean, labels):\n",
    "    correct = np.argmax(prob_mean, axis=1) == np.argmax(labels, axis = 1)\n",
    "    correct_indices = [i for i in range(prob_mean.shape[0]) if correct[i]]\n",
    "    incorrect_indices = [i for i in range(prob_mean.shape[0]) if not correct[i]]\n",
    "    return correct_indices, incorrect_indices\n",
    "\n",
    "def plot_entropy_distribution(prob_mean, labels):\n",
    "    entropy = -np.sum(prob_mean * np.log2(prob_mean), axis=1)\n",
    "    corr_indices, incorr_indices = get_correct_indices(prob_mean, labels)\n",
    "    indices = [corr_indices, incorr_indices]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    for i, category in zip(range(2), ['Correct', 'Incorrect']):\n",
    "        entropy_category = np.array([entropy[j] for j in indices[i]])\n",
    "        mean_entropy = np.mean(entropy_category[~np.isnan(entropy_category)])\n",
    "        num_samples = entropy_category.shape[0]\n",
    "        #title = category + 'ly labelled ({:.2f}% of total)'.format(num_samples / x.shape[0] * 100)\n",
    "        title = category + 'ly labelled'.format(num_samples / x.shape[0] * 100)\n",
    "        axes[i].hist(entropy_category, weights=(1/num_samples)*np.ones(num_samples))\n",
    "        axes[i].annotate('Mean: {:.3f}'.format(mean_entropy), (0.4, 0.9), ha='center')\n",
    "        axes[i].set_xlabel('Entropy')\n",
    "        axes[i].set_ylim([0, 1])\n",
    "        #axes[i].set_ylabel('Probability')\n",
    "        axes[i].set_title(title)\n",
    "        print(num_samples)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXq-yrZtWNpN"
   },
   "outputs": [],
   "source": [
    "# Generate ensemble of predictions from the BNN\n",
    "\n",
    "ensemble_size = 200\n",
    "x = X_test_scaled\n",
    "\n",
    "prob_predictions = np.empty(shape=(ensemble_size, 40328, 6))\n",
    "for i in range(ensemble_size):\n",
    "    prob_predictions[i] = bnn_model(x).mean().numpy()\n",
    "\n",
    "prob_mean = prob_predictions.mean(axis = 0)\n",
    "\n",
    "# reshape array\n",
    "pred = np.nan * np.zeros((360*720))\n",
    "pred[maskTest.flatten()] = prob_mean.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1oO3Cy9MPkt"
   },
   "outputs": [],
   "source": [
    "# find indices of datapoints for which the neural network is correct and indices for which it is incorrect\n",
    "c_in, inc_in = get_correct_indices(prob_mean, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTZ8EHBVLieC"
   },
   "source": [
    "#### Example of two output distributions predicted by BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbtIZC8JLfxJ"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (6.4, 4.8)\n",
    "\n",
    "## two correct prediction\n",
    "corr_0 = boxplot_model_predictions(prob_predictions, c_in[107*200], test_label, run_ensemble = True)\n",
    "corr_1 = boxplot_model_predictions(prob_predictions, c_in[75*200], test_label, run_ensemble = True)\n",
    "\n",
    "## two incorrect predictions\n",
    "incorr_0 = boxplot_model_predictions(prob_predictions, inc_in[100], test_label, run_ensemble = True)\n",
    "incorr_1 = boxplot_model_predictions(prob_predictions, inc_in[200], test_label, run_ensemble = True)\n",
    "\n",
    "# Note in these box and whisker plots the distributions themselves represent the aleatoric uncertainty \n",
    "# and the box and whiskers (ie. the ensemble of possible distributions) represent the epistemic uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6KOhQ-9yVQy"
   },
   "source": [
    "#### Uncertainty quantification using entropy\n",
    "\n",
    "Model uncertainty can be quantified by calculating the [entropy](https://en.wikipedia.org/wiki/Entropy_%28information_theory%29) of the distribution. The higher the value, the more unsure the model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install Basemap # note you may have this already installed and therefore do not need to run this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAmajayW0NJ6"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import * \n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "entropy = -np.sum(prob_mean* np.log2(prob_mean), axis=1)\n",
    "all_results = np.nan * np.zeros((360*720))\n",
    "all_results[maskTest.flatten()] = entropy\n",
    "lat = monthly_ssh['lat']\n",
    "lon = monthly_ssh['lon']\n",
    "lons = lon[1,:].values\n",
    "lats = lat[:,1].values\n",
    "llons, llats = np.meshgrid(lons,lats)\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "m = Basemap(llcrnrlon=-80, urcrnrlon=20, llcrnrlat=-80, urcrnrlat=89, projection='mill', resolution='l')\n",
    "m.drawmapboundary(fill_color='0.9')\n",
    "m.drawparallels(np.arange(-90.,99.,30.),labels=[1,1,0,1])\n",
    "m.drawmeridians(np.arange(-180.,180.,60.),labels=[0,0,0,1])\n",
    "m.drawcoastlines()\n",
    "m.fillcontinents()\n",
    "im1 = m.pcolor(llons, llats, np.flipud(np.reshape(all_results,(360,720)))[::-1,:], latlon=True, cmap = plt.cm.Oranges)\n",
    "cbar = plt.colorbar(pad=0.075)\n",
    "cbar.set_label('Entropy')\n",
    "im2 = m.scatter([-63.25], [23.75], marker = 'D', c = 'black',  s = 50, latlon = True)\n",
    "im3 = m.scatter([-4.25], [-12.75], marker = 'D', c = 'blue',  s = 50, latlon = True)\n",
    "im4 = m.scatter([-37.25], [-17.25], marker = 'D', c = 'magenta', s = 50, latlon = True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xDbX_tRHGeg"
   },
   "outputs": [],
   "source": [
    "## For a robust trustworthy BNN, we would expect the correct predictions to have a low entropy ie. a low uncertainty, and the incorrect predictions to have a\n",
    "## high entropy ie. a high uncertainty. \n",
    "## This is desirable because in simple terms, it means that the BNN is more certain of its prediction when it is correct than when it is incorrect\n",
    "plot_entropy_distribution(prob_mean, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7Z1DHABij_l"
   },
   "source": [
    "There are also many other interesting plots that can be produced from BNN predictions as shown in Clare et al. 2022.\n",
    "The code to make these plots is at the end of this [notebook on github](https://github.com/maikejulie/DNN4Cli/blob/main/THOR/BayesianApproach/THOR%20-%20Step_2_Bayesian_approach.ipynb) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

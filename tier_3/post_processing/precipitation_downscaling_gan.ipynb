{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hhBqIq-0s3N"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CP6g74dA2hgA"
   },
   "source": [
    "In many applications, we get precipitation fields at lower resolution than we need. For example, climate models usually have resolution in the tens of kilometers. This makes it difficult to study the effects of climatic change on extreme precipitation, which usually happens at shorter length scales. To remedy this, we can use *downscaling*: statistical methods that create higher-resolution fields from low-resolution inputs. These are related to the so-called *super-resolution* methods in image processing.\n",
    "\n",
    "Generative Adversarial Networks are naturally suited for downscaling because they can learn to create realistic-looking outputs. In this notebook, we'll use TensorFlow/Keras to build a simple conditional GAN for downscaling radar-measured precipitation fields. You can train the GAN yourself, or if you prefer, you can jump straight into generating fields yourself.\n",
    "\n",
    "This notebook is based on the research article [\"Stochastic Super-Resolution for Downscaling Time-Evolving Atmospheric Fields With a Generative Adversarial Network\"](https://doi.org/10.1109/TGRS.2020.3032790), which used a dataset from the MeteoSwiss radar network. To make the dataset a more manageable size and reduce the cost of training, the time dimension used in that paper has been removed, the sample size reduced to 32x32, 64000 training samples have been randomly selected from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG1E5jTav6zC"
   },
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb-msoQEwAHz"
   },
   "source": [
    "To work with this notebook, you'll need it to run in an environment with TensorFlow, NumPy, Matplotlib and NetCDF4 installed. You should also have a GPU available. One option is to run this on [Google Colab](https://colab.research.google.com/github/ecmwf-projects/mooc-machine-learning-weather-climate/blob/main/tier_3/post_processing/precipitation_downscaling_gan.ipynb), where these packages are already installed, except netCDF4, which can be installed by running the code in the cell below. If using Colab, go to Edit -> Notebook settings -> Hardware accelerator and select \"GPU\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install netCDF4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7SnwImD0ywT"
   },
   "source": [
    "# Setting up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RdnhsJ6JMkX"
   },
   "source": [
    "**Loading the high-resolution data**\n",
    "\n",
    "We start by downloading the precipitation data and reading it to memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWeVHYEkKROb"
   },
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import numpy as np\n",
    "\n",
    "# Download data to our local directory\n",
    "!wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_3/precipitation_downscaling/scale_rzc.txt\n",
    "!wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/tier_3/precipitation_downscaling/samples-2018-32x32.nc\n",
    "\n",
    "# Load the samples\n",
    "with netCDF4.Dataset(\"samples-2018-32x32.nc\", 'r') as ds:\n",
    "    samples = np.array(ds[\"samples\"][:], copy=False)\n",
    "\n",
    "# the original data is in an 8-bit format that needs to be scaled\n",
    "scale = np.loadtxt(\"scale_rzc.txt\").astype(np.float32)\n",
    "samples = scale[samples]\n",
    "\n",
    "# convert missing data to zeros\n",
    "samples[~np.isfinite(samples)] = 0\n",
    "\n",
    "# Do a log-transformation to reduce dynamic range.\n",
    "# We threshold data to 0.1 mm/h and set everything below to 0.02 mm/h\n",
    "# to allow the log-transformation to work.\n",
    "samples[samples < 0.1] = 0.02 \n",
    "samples = np.log10(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFOmDub8KlSl"
   },
   "source": [
    "**Creating input datasets**\n",
    "\n",
    "For this demonstration work, we simply create the low-resolution samples by spatially averaging our high-resolution samples. We use a 4x4 size for the low resolution, so our downscaling will have a resolution multiplier of 8. Then we pack both the high-resolution and low-resolution data in a TensorFlow `Dataset`.\n",
    "\n",
    "If you see an `ImportError` while running the code, make sure you have TensorFlow installed in your notebook environment. For instance, on Google Colab you'll need to switch to a GPU node (see the Getting started\" section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlJQKZUULGTj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "rf = 8 # resolution factor\n",
    "\n",
    "# Create low-resolution (LR) array\n",
    "lr_shape = (\n",
    "    samples.shape[0], samples.shape[1]//rf,\n",
    "    samples.shape[2]//rf, samples.shape[3]\n",
    ")\n",
    "\n",
    "# 8x8 average pooling\n",
    "samples_lr = np.zeros(lr_shape, samples.dtype)\n",
    "for i in range(rf):\n",
    "    for j in range(rf):\n",
    "        samples_lr += samples[:,i::rf,j::rf,:]\n",
    "samples_lr /= rf**2\n",
    "\n",
    "# set aside 10% of data for testing (not used to train model)\n",
    "indices = np.arange(samples.shape[0])\n",
    "rng = np.random.RandomState(seed=1)\n",
    "rng.shuffle(indices)\n",
    "N_testing = int(samples.shape[0]*0.1)\n",
    "samples_test = samples[indices[:N_testing],...]\n",
    "samples = samples[indices[N_testing:],...]\n",
    "samples_lr_test = samples_lr[indices[:N_testing],...]\n",
    "samples_lr = samples_lr[indices[N_testing:],...]\n",
    "\n",
    "# create TensorFlow Dataset for training\n",
    "batch_size = 64\n",
    "dataset = tf.data.Dataset.from_tensor_slices((samples_lr, samples))\n",
    "dataset = dataset.shuffle(buffer_size=256).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAd3xJ460IB8"
   },
   "source": [
    "# Plotting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w09X-9fE0LWX"
   },
   "source": [
    "Let's take a quick look at the data. First, we define some functions for plotting using Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8kWrVt7W8sB"
   },
   "outputs": [],
   "source": [
    "from matplotlib import colors, gridspec, pyplot as plt\n",
    "\n",
    "def plot_precip(ax, logR):\n",
    "    \"\"\" Plot a single precipitation image.\n",
    "    \"\"\"\n",
    "    if logR.ndim == 3:\n",
    "        logR = logR[:,:,0] # remove channels dimension\n",
    "    R = 10**logR\n",
    "    R[R < 0.1] = np.nan\n",
    "    ax.imshow(R, norm=colors.LogNorm(0.1,100,clip=True))\n",
    "    ax.tick_params(left=False, bottom=False,\n",
    "        labelleft=False, labelbottom=False)\n",
    "\n",
    "def plot_samples(samples, labels=None):\n",
    "    \"\"\" Plot a grid with many images.\n",
    "    \"\"\"\n",
    "    N_variables = len(samples)\n",
    "    N_samples = len(samples[0])\n",
    "    fig = plt.figure(figsize=(N_samples*1.5, N_variables*1.5))\n",
    "    gs = gridspec.GridSpec(N_variables, N_samples, hspace=0.02, wspace=0.02)\n",
    "\n",
    "    for (i,variable) in enumerate(samples):\n",
    "        for (j,sample) in enumerate(variable):\n",
    "            ax = fig.add_subplot(gs[i,j])\n",
    "            plot_precip(ax, sample)\n",
    "            if (j == 0) and (labels is not None):\n",
    "                ax.set_ylabel(labels[i])\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvKTDjiq0U0S"
   },
   "source": [
    "Then, we pick a few interesting examples from the test dataset and show both the high- and low-resolution versions.\n",
    "\n",
    "If you want to explore the dataset more, change the `indices` variable or uncomment the lines that pick the indices randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqDBCU3vZC10"
   },
   "outputs": [],
   "source": [
    "# Show a selection of interesting samples from the test set\n",
    "indices = [2,5,14,16,17,19,28,37]\n",
    "\n",
    "# Uncomment to use a random selection (change seed for a different selection):\n",
    "# rng = np.random.RandomState(seed=4)\n",
    "# indices = rng.choice(samples_test.shape[0], 8, replace=False)\n",
    "\n",
    "plot_samples(\n",
    "    [samples_test[indices,...], samples_lr_test[indices,...]],\n",
    "    labels=[\"HR samples\", \"LR samples\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-8gRoUMzqgX"
   },
   "source": [
    "# Creating the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfxHa2GTOeDR"
   },
   "source": [
    "**GAN networks**\n",
    "\n",
    "Here, we create the generator and discriminator networks. We use a fairly simple upsampling convolutional neural network (ConvNet) for the generator and a downsampling ConvNet for the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiI5BbeoPQVP"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Concatenate, Conv2D, Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Input, UpSampling2D\n",
    "\n",
    "\n",
    "def create_generator(\n",
    "    block_widths=(128,64,32),\n",
    "    input_shape=(4,4,1),\n",
    "    noise_shape=(4,4,8)\n",
    "):\n",
    "    # inputs (concatenated)\n",
    "    input_lr = Input(shape=input_shape)\n",
    "    input_noise = Input(shape=noise_shape)\n",
    "    x = Concatenate()([input_lr, input_noise])\n",
    "\n",
    "    x = Conv2D(\n",
    "        block_widths[0], kernel_size=(3,3), padding='same'\n",
    "    )(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    # upsampling stages\n",
    "    for width in block_widths:\n",
    "        x = UpSampling2D(interpolation='bilinear')(x)\n",
    "        x = Conv2D(\n",
    "            width, kernel_size=(3,3), padding='same'\n",
    "        )(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    # final projection to output space\n",
    "    output = Conv2D(1, kernel_size=(1,1))(x)\n",
    "\n",
    "    return Model(inputs=[input_lr, input_noise], outputs=output)\n",
    "\n",
    "\n",
    "def create_discriminator(\n",
    "    block_widths=(32,64,128),\n",
    "    input_shape_hr=(32,32,1),\n",
    "    input_shape_lr=(4,4,1),\n",
    "    postproc_convs=2,\n",
    "    output_activation='linear' # 'linear' when using 'from_logits' in loss\n",
    "):\n",
    "    # inputs\n",
    "    input_hr = Input(shape=input_shape_hr)\n",
    "    input_lr = Input(shape=input_shape_lr)\n",
    "\n",
    "    # Convolution steps for both HR and LR inputs\n",
    "    x_hr = Conv2D(\n",
    "        block_widths[0], kernel_size=(3,3), padding='same', activation='relu'\n",
    "    )(input_hr)\n",
    "    x_hr = LeakyReLU(0.2)(x_hr)\n",
    "    x_lr = Conv2D(\n",
    "        block_widths[0], kernel_size=(3,3), padding='same', activation='relu'\n",
    "    )(input_lr)\n",
    "    x_lr = LeakyReLU(0.2)(x_lr)\n",
    "    for width in block_widths:\n",
    "        x_hr = Conv2D( # downsample HR samples on each loop\n",
    "            width, kernel_size=(3,3), padding='same', strides=2,\n",
    "        )(x_hr)\n",
    "        x_hr = LeakyReLU(0.2)(x_hr)\n",
    "        x_lr = Conv2D(\n",
    "            width, kernel_size=(3,3), padding='same'\n",
    "        )(x_lr)\n",
    "        x_lr = LeakyReLU(0.2)(x_lr)\n",
    "\n",
    "    # Concatenate HR and LR branches and do some joint processing\n",
    "    x = Concatenate()([x_hr, x_lr])\n",
    "    for _ in range(postproc_convs):\n",
    "        x = Conv2D(block_widths[-1], kernel_size=(3,3), padding='same')(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    # Pool to get outputs\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    output = Dense(1, activation=output_activation)(x)\n",
    "\n",
    "    return Model(inputs=[input_lr, input_hr], outputs=output)\n",
    "\n",
    "\n",
    "generator = create_generator()\n",
    "discriminator = create_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xT2vVhdNVuY_"
   },
   "outputs": [],
   "source": [
    "# Print summaries of the model\n",
    "generator.summary()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pb_HPb3iU5G1"
   },
   "source": [
    "**GAN training loop**\n",
    "\n",
    "We create a custom Keras model for the GAN logic. We override `train_step` to implement the nonstandard GAN training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_38rHkfU9qY"
   },
   "outputs": [],
   "source": [
    "# Adapted from https://keras.io/examples/generative/conditional_gan/\n",
    "\n",
    "class DownscalingGAN(Model):\n",
    "    def __init__(self, discriminator, generator, noise_shape=(4,4,8)):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.noise_shape = noise_shape\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "        self.step_number = tf.Variable(0)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        (samples_lr, samples_hr) = data\n",
    "\n",
    "        # Sample noise for the generator.\n",
    "        batch_size = tf.shape(samples_hr)[0]\n",
    "        noise = tf.random.normal(\n",
    "            shape=(batch_size,)+self.noise_shape\n",
    "        )\n",
    "\n",
    "        def train_discriminator():\n",
    "            # Generate fake samples.\n",
    "            samples_gen = self.generator([samples_lr, noise])\n",
    "\n",
    "            # Combine them with real images.\n",
    "            combined_samples_hr = tf.concat([samples_gen, samples_hr], axis=0)\n",
    "            combined_samples_lr = tf.concat([samples_lr, samples_lr], axis=0)\n",
    "\n",
    "            # Assemble labels discriminating real from fake images.\n",
    "            labels = tf.concat(\n",
    "                [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "            )\n",
    "\n",
    "            # Train the discriminator.\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = self.discriminator(\n",
    "                    [combined_samples_lr, combined_samples_hr]\n",
    "                )\n",
    "                d_loss = self.loss_fn(labels, predictions)\n",
    "            grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(grads, self.discriminator.trainable_weights)\n",
    "            )\n",
    "\n",
    "            # Monitor loss.\n",
    "            self.disc_loss_tracker.update_state(d_loss)\n",
    "\n",
    "        def train_generator():\n",
    "            # Create labels that say \"all real images\".\n",
    "            misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "            # Train the generator (note that we should *not* update the weights\n",
    "            # of the discriminator)!\n",
    "            with tf.GradientTape() as tape:\n",
    "                samples_gen = self.generator([samples_lr, noise])\n",
    "                predictions = self.discriminator([samples_lr, samples_gen])\n",
    "                g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "            grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "            self.g_optimizer.apply_gradients(\n",
    "                zip(grads, self.generator.trainable_weights)\n",
    "            )\n",
    "\n",
    "            # Monitor loss.\n",
    "            self.gen_loss_tracker.update_state(g_loss)\n",
    "        \n",
    "        # If we trained the generator now, train the discriminator next\n",
    "        # or vice versa.\n",
    "        \n",
    "        tf.cond(\n",
    "            self.step_number % 2 == 0,\n",
    "            train_discriminator,\n",
    "            train_generator\n",
    "        )        \n",
    "        self.step_number.assign(self.step_number+1)\n",
    "\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "gan = DownscalingGAN(discriminator, generator)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JroyiEkzTV-"
   },
   "source": [
    "# Training the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwYkKbfSKu9T"
   },
   "source": [
    "Now, training the GAN is really simple! We start it just like we would any Keras model training - with a call to `fit`.\n",
    "\n",
    "We train for 30 epochs which should be enough to get reasonable looking results in about 15 minutes. To train more, you can increase `epochs` - or just run the cell below again.\n",
    "\n",
    "While your waiting for the GAN to train, you can take some time to **watch the video** belonging to this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQwsYwFTKxGi"
   },
   "outputs": [],
   "source": [
    "gan.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-nt4RAZ1ekc"
   },
   "source": [
    "# Examining the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04weCZFov5u6"
   },
   "source": [
    "**Plotting generated samples**\n",
    "\n",
    "With our GAN trained, we can now create some downscaled precipitation fields! Run the code below to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IO3btn6O7Cw"
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=15)\n",
    "num_samples = 8\n",
    "\n",
    "# Selection of interesting samples from the test set\n",
    "indices = [2,5,14,16,17,19,28,37]\n",
    "\n",
    "# Uncomment to use a random selection (change seed for a different selection):\n",
    "# indices = rng.choice(samples.shape[0], num_samples, replace=False)\n",
    "\n",
    "noise = rng.randn(num_samples, 4, 4, 8)\n",
    "samples_gen_1 = generator.predict_on_batch(\n",
    "    [samples_lr_test[indices,...], noise]\n",
    ")\n",
    "noise = rng.randn(num_samples, 4, 4, 8)\n",
    "samples_gen_2 = generator.predict_on_batch(\n",
    "    [samples_lr_test[indices,...], noise]\n",
    ")\n",
    "plot_samples(\n",
    "    [\n",
    "        samples_lr_test[indices,...], samples_test[indices,...], \n",
    "        samples_gen_1, samples_gen_2    \n",
    "    ],\n",
    "    labels=[\"LR samples\", \"HR samples\", \"Gen. samples 1\", \"Gen. samples 2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kihHqI9WwXKp"
   },
   "source": [
    "The first row shows the low-resolution precipitation samples, the next shows the real high-resolution precipitation, and the bottom two rows show downscaled fields from the GAN. We can see that the GAN creates reasonable guesses of what the high-resolution precipitation fields might look like. If you look at the generated images more closely, you'll also see that there are small differences in the two GAN-generated fields. This means that our GAN is generating solutions stochastically - you'll get a different guess every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0905cCH1kk1"
   },
   "source": [
    "# Optional homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1vpKTBfxoqJ"
   },
   "source": [
    "Here are some suggestions if you'd like to spend some more time to study the GAN in more detail:\n",
    "* Train the GAN longer (increase `epochs` in `gan.fit` and see if the generated results get better).\n",
    "* Try to adjust the generator and discriminator architectures and see what effect it has on the images.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

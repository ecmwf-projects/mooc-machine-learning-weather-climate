{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f46ed5a",
   "metadata": {},
   "source": [
    "  <h1><center>ECMWF MOOC MACHINE LEARNING IN WEATHER AND CLIMATE</center></h1>\n",
    "\n",
    "   <h2><center>Tier 3: Practical ML application in weather and climate - OBSERVATIONS</center></h2>\n",
    "   \n",
    "   <h2><center> Exercise on how to build a simple ANN model for satellite precipitation retrieval using passive microwave measurements </center></h2>\n",
    "\n",
    " <h3><center>Created by: Daniele Casella, Paolo Sanò, Leo Pio D'Adderio, Riccardo D'Ercole </center></h3>\n",
    "  <h3><center> CNR-ISAC, Rome Italy</center></h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abbf960",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This Jupiter Notebook is on how to build a simple Artificialt Neural Network (ANN) (Multilayer Perceptrons) for surface precipitation rate estimation using the GPM Microwave Imager (GMI) measurements as input. The training dataset is a small-size osbervational dataset built from coincident measurements of the GMI and the Dual-frequency Precipitation Radar (DPR) on board the NASA/JAXA GPM-Core Observatory. The ANN architecture is pre-defined and an optimal set of parameters for the training phase is provided. The goal is to define the ANN (the weights) by minimising the prediction error (the loss function) avoiding overfitting during training. As you go through this Jupiter Notebook, you will learn about GMI characteristics, definition of ANN architecture, key parameters in the training phase, minimisation of the loss function, and overfitting. The ANN model obtained will be applied to one case study. \n",
    "\n",
    "As First Task it is recommended that you go through the code and the plots provided, understanding the meaning of each step. As a second Task you may run the code, play with the parameters, and analyse the results. Please, consider that the training for each set of parameters can take a few minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42079fd",
   "metadata": {},
   "source": [
    "## Before starting: python and libraries installation\n",
    "\n",
    "### Python\n",
    "Python 3 is required for running this code on your machine. The fastest way to have python is to install the [Anaconda Package](https://www.anaconda.com/products/individual).  By the way, python can be installed on all OS, there are versions of Anaconda for Linux Mac and Windows, however this code has been tested for Linux Ubuntu and windows 64 bit OS. \n",
    "\n",
    "Before starting you need to install some additional software and some python packages.\n",
    "\n",
    "\n",
    "### Libraries installation \n",
    "\n",
    "The easiest way to get everything installed is to use conda (from a terminal or Anaconda Prompt in Windows) to create a new virtual environment:\n",
    "\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\"> \n",
    "  user@user:/home/user$ conda create -n tf tensorflow pandas scikit-learn matplotlib basemap xarray netCDF4 h5netcdf h5py\n",
    "  user@user:/home/user$ conda activate tf\n",
    "</code>\n",
    "</p>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed43214",
   "metadata": {},
   "source": [
    "# GMI - GPM Microwave Imager\n",
    "The GMI is the most advanced spaceborne MW radiometer currently available, operating on the NASA/JAXA GPM Core Observatory (GPM-CO) launched in March 2014 (https://gpm.nasa.gov/). The GMI is a conical-scanning radiometer measuring vertically (V) and horizontally (H) polarised radiances in 13 channels between 10.65 and 183.31 GHz. Table 1 presents some details on GMI channels and their properties. In each channel the measured radiance, which is converted into brightness temperature (TB) for physical interpretation, results from the interaction of surface-emitted radiation with atmospheric water vapour and the liquid and solid hydrometeros within the cloud. Its 13 channels have different penetration ability in the cloud and are sensitive to different hydrometeor types and precipitation intensities. Complex retrieval algorithms convert the multichannel TBs in surface precipitation rate. There are several factors which makes the quantitative precipitation estimation from PMW radiometers (and from GMI) very challenging. Moreover, the relationhip between the multichannel TBs and surface rain rate is indirect and highly non linear, and machine learning techniques can be used to overcome some of these issues (e.g., Sanò et al., doi:10.3390/rs10071122) .\n",
    "\n",
    "\n",
    " No. | Central frequency (GHz) | Polarisation | IFOV size \n",
    " :-: | :-: | :-: | :-: \n",
    " 1 | 10.65 | V | 19x32 km\n",
    " 2 | 10.65 | H | 19x32 km\n",
    " 3 | 18.7 | V | 11x18 km\n",
    " 4 | 18.7 | H | 11x18 km\n",
    " 5 | 23.8 | V | 9.2x15 km\n",
    " 6 | 36.5 | V | 8.6x14 km\n",
    " 7 | 36.5 | H | 8.6x14 km\n",
    " 8 | 89.0 | V | 4.4x7.2 km\n",
    " 9 | 89.0 | H | 4.4x7.2 km \t\n",
    "10 | 166.5 | V | 4.4x7.2 km\n",
    "11 | 166.5 | H | 4.4x7.2 km\t\n",
    "12 | 183.31 ± 3 | V | 4.4x7.2 km\n",
    "13 | 183.31 ± 7 | V | 4.4x7.2 km\t\n",
    "\n",
    "<center><i>Table 1. GMI channels characteristics</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfd5a77",
   "metadata": {},
   "source": [
    "Now, we are ready to read the GMI files. The GMI files format is HDF5, and the \"h5py\" library has to be imported in order to be able to read the data. At the following link <a href=\"https://gpm.nasa.gov/resources/documents/file-specification-gpm-products/\">GPM File Specification</a> you can find the document explaining the GMI file structure as well as the file structure of all Level 1 to Level 3 GPM products. The GPM products (including GMI) can be downloaded from <a href=\"https://storm.pps.eosdis.nasa.gov/storm/\"> the NASA PPS Storm </a> website. In the following block we see how to open and read the content of a HDF5 file (i.e. the GMI file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c21822",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c518d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "gmi_file_path='./1C-R.GPM.GMI.XCAL2016-C.20200916-S130832-E144106.037225.V07A.HDF5'\n",
    "hf=h5py.File(gmi_file_path,'r')\n",
    "dataset1=hf['S1']\n",
    "print(dataset1.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a5582d",
   "metadata": {},
   "source": [
    "### GMI TB Imagery\n",
    "In the following block, we are going to see how to create TB imagery for selected GMI channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the function to plot\n",
    "def plot_tb_map(TB,lat,lon,colorAxisMin,colorAxisMax):\n",
    "    # set the lat/lon borders of the map\n",
    "    latborders=[34,40]\n",
    "    lonborders=[14,22]\n",
    "    m = Basemap(projection='merc',\n",
    "        resolution='l',\n",
    "        lat_ts=20,\n",
    "        llcrnrlat=latborders[0],urcrnrlat=latborders[1],\\\n",
    "        llcrnrlon=lonborders[0],urcrnrlon=lonborders[1])\n",
    "\n",
    "    \n",
    "#E set min/max values of the colorbar\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        im1 = m.pcolor(lon[:],lat[:],TB[:],shading='nearest',cmap='turbo',latlon=True,vmin=colorAxisMin,vmax=colorAxisMax)\n",
    "    m.drawcoastlines()\n",
    "    \n",
    "# set parallels and meridians\n",
    "    dparal=2 #separation in deg between drawn parallels\n",
    "    parallels = np.arange(latborders[0],latborders[1],dparal)\n",
    "    dmerid=2 #separation in deg between drawn meridians\n",
    "    meridians = np.arange(lonborders[0],lonborders[1],dmerid)\n",
    "    m.drawparallels(parallels,labels=[1,0,0,0],fontsize=15)\n",
    "    m.drawmeridians(meridians,labels=[0,0,0,1],fontsize=15) \n",
    "# add colorbar.\n",
    "    cbar = m.colorbar(location='right',pad=\"5%\")\n",
    "    cbar.set_label('K')\n",
    "\n",
    "\n",
    "\n",
    "#windows users may need to set an environmental variable before\n",
    "#import os\n",
    "#os.environ[\"PROJ_LIB\"] = 'C:'+os.sep+'Users'+os.sep+'Leo'+os.sep+'anaconda3'+os.sep+'Library'+os.sep+'share'; #with your anaconda installation path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap \n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "import h5py\n",
    "import warnings\n",
    "\n",
    "gmi_file_path='./1C-R.GPM.GMI.XCAL2016-C.20200916-S130832-E144106.037225.V07A.HDF5'\n",
    "hf=h5py.File(gmi_file_path,'r')\n",
    "\n",
    "dataset1=hf['S1']\n",
    "dataset2=hf['S2']\n",
    "lat1=hf['S1/Latitude'][:]\n",
    "lon1=hf['S1/Longitude'][:]\n",
    "TB=hf['S1/Tc'][:]\n",
    "TB10v=TB[:,:,0]\n",
    "TB10h=TB[:,:,1]\n",
    "TB19v=TB[:,:,2]\n",
    "TB19h=TB[:,:,3]\n",
    "TB23v=TB[:,:,4]\n",
    "TB37v=TB[:,:,5]\n",
    "TB37h=TB[:,:,6]\n",
    "TB89v=TB[:,:,7]\n",
    "TB89h=TB[:,:,8]\n",
    "\n",
    "lat2=hf['S2/Latitude'][:]\n",
    "lon2=hf['S2/Longitude'][:]\n",
    "\n",
    "\n",
    "TB=hf['S2/Tc'][:]\n",
    "TB166v=TB[:,:,0]\n",
    "TB166v[TB166v<0]=np.NaN\n",
    "TB166h=TB[:,:,1]\n",
    "hf.close()\n",
    "\n",
    "fig = plt.figure(figsize=(24,16))\n",
    "ax = fig.add_subplot(231)\n",
    "plot_tb_map(TB10v,lat1,lon1,colorAxisMin=160,colorAxisMax=280)\n",
    "plt.title('TB 10v')\n",
    "\n",
    "ax = fig.add_subplot(232)\n",
    "plot_tb_map(TB19v,lat1,lon1,colorAxisMin=180,colorAxisMax=280)\n",
    "plt.title('TB 19v')\n",
    "\n",
    "ax = fig.add_subplot(233)\n",
    "plot_tb_map(TB23v,lat1,lon1,colorAxisMin=200,colorAxisMax=300)\n",
    "plt.title('TB 23v')\n",
    "\n",
    "ax = fig.add_subplot(234)\n",
    "plot_tb_map(TB37v,lat1,lon1,colorAxisMin=210,colorAxisMax=280)\n",
    "plt.title('TB 37v')\n",
    "\n",
    "ax = fig.add_subplot(235)\n",
    "plot_tb_map(TB89v,lat1,lon1,colorAxisMin=130,colorAxisMax=280)\n",
    "plt.title('TB 89v')\n",
    "\n",
    "ax = fig.add_subplot(236)\n",
    "plot_tb_map(TB166v,lat1,lon1,colorAxisMin=140,colorAxisMax=300)\n",
    "plt.title('TB 166v')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b8b67",
   "metadata": {},
   "source": [
    "\n",
    "The panels show the GMI brightness temperatures (TBs) for the overpass which captured the Mediterranean Hurricane (Medicane) named Ianos on 16/09/2020 over the Ionian Sea between Southern Italy and Greece.\n",
    "The first row of panels shows the low frequency channels (< 30 GHz) affected mostly by the emission of radiation by the rain droplets, which results in higher TBs with respect to the radiatively cold sea surface background (warm areas in the imagery are associated to the precipitation in the rain bands around the cyclone center). The second row of panels shows the higher frequency channels, where the effect of the scattering by the ice on the observed TBs becomes visible as cold areas in the imagery. The scattering signal at 37 GHz and 89 GHz channel (low TBs) are due to the larger (and denser) frozen hydrometeors (hail/graupel) found in the convective cores, while at higher frequency the scattering is due to the smaller size/less dense ice in the upper cloud layers.  The following table summaries the properties of the six GMI frequencies (for further details please refer to D'Adderio et al.,10.1016/j.atmosres.2022.106174).   \n",
    "\n",
    " <table>\n",
    "    \n",
    "  <tr>\n",
    "    <th style=\"text-align:center\"> GMI channel</th>\n",
    "    <th style=\"text-align:center\"> TB warmer than background</th>\n",
    "    <th style=\"text-align:center\"> TB colder than background</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:center\"> 10 GHz</td>\n",
    "    <td style=\"text-align:center\"> emission from large raindrops (lower rain layers) </td>\n",
    "    <td style=\"text-align:center\"> - </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:center\"> 19 GHz</td>\n",
    "    <td style=\"text-align:center\"> emission from large raindrops (rain)</td>\n",
    "    <td style=\"text-align:center\"> - </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:center\"> 22 GHz</td>\n",
    "    <td style=\"text-align:center\"> emission from large raindrops (rain)<br>emission from  water vapour</td>\n",
    "    <td style=\"text-align:center\"> - </td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td style=\"text-align:center\"> 37 GHz</td>\n",
    "    <td style=\"text-align:center\"> emission from raindrops (rain)</td>\n",
    "    <td style=\"text-align:center\"> Scattering by large and dense ice<br>(e.g. hail – deep convection)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:center\"> 89 GHz</td>\n",
    "    <td style=\"text-align:center\"> emission from water vapour<br>and cloud liquid water</td>\n",
    "    <td style=\"text-align:center\"> Scattering by precipitating heavily rimed ice<br>(e.g., graupel – convection/deep convection)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td style=\"text-align:center\"> 166 GHz</td>\n",
    "    <td style=\"text-align:center\"> emission from water vapour<br>and cloud liquid water</td>\n",
    "    <td style=\"text-align:center\"> Scattering by less dense ice<br>(snowflakes and aggregates – stratiform/convective precip)</td>\n",
    "  </tr>\n",
    "</table> \n",
    "<center><i>Table 2. Properties of some GMI channels related to clouds and precipitation.</i></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d628c4e5",
   "metadata": {},
   "source": [
    "# Exercise: training a simple artificial neural network model\n",
    "\n",
    "This section explains how to train a Multilayer Perceptrons (MLP) Neural Network which predicts the target variable or label (surface rain rate) given a set of 13 pre-defined inputs or features (TBs). The model represents a simple Neural Network (NN) with two hidden layers and a final layer which is used to predict the target label.\n",
    "\n",
    "The section is composed by the following parts:\n",
    "\n",
    "1) Data loading and conversion\n",
    "<br>\n",
    "2) Dataset description\n",
    "<br>\n",
    "3) Training and test data split\n",
    "<br>\n",
    "4) Model training \n",
    "<br>\n",
    "5) Model evaluation\n",
    "<br>\n",
    "6) Further training of the model and overfitting\n",
    "<br>\n",
    "7) Testing the trained model on the initial case study\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning </b> The neural network that will be trained during this exercise is not suitable for scientific/operational purposes, the main limitation comes from the training dataset, which is very small (10 orbits) with respect to the ones used in scientific/operational algorithms, which can include hundreds of thousands of orbits and millions of pixels. Using very large training dataset is of paramount importance for obtaining a model that is general, i.e. a model that performs with the same accuracy when applied to the training or to an independent dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b3efe",
   "metadata": {},
   "source": [
    "### 1) Data loading and conversion\n",
    "First, we will import all the libraries which are necessary to this exercise.\n",
    "<br>\n",
    "After loading the data in xarray format, which is one of the most common Python libraries to process netcdf files, the data is transformed into tensors which is the standard format for training Machine Learning or Deep Learning models. \n",
    "<br> For the purpose of this exercise, we will use the Tensorflow framework, which is built on top of Keras Python library in order to train a Neural Network.\n",
    "<br> We observe that the features dataset have 61906 different records and 13 features, whereas the label dataset consist of one variable and 61906 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ce60e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import utils\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data_path = './dataset2_GMI_DPR_RR.nc'\n",
    "\n",
    "ds = xr.open_dataset(data_path)\n",
    "\n",
    "train_df = ds['tb'].to_dataframe().unstack()\n",
    "target = ds['rr'].to_dataframe()\n",
    "\n",
    "print('The shape of the features data is', train_df.shape)\n",
    "print('The shape of the label data is', target.shape)\n",
    "\n",
    "tensor_df = tf.convert_to_tensor(train_df, dtype=np.float)\n",
    "label_df = tf.convert_to_tensor(target, dtype=np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f43b0",
   "metadata": {},
   "source": [
    "### 2) Dataset description\n",
    "\n",
    "The dataset used for training and test of the NN is built from 10 GMI orbits (from 9th March 2014). The 13 features correspond exactly to the 13 GMI channels taken in the same order as in Table 1. The rainfall rate represents the target variable and has been obtained from the NASA GMI/DPR Level 2 precipitation product (2B-CMB). This algorithm combines GMI measurements with the reflectivity profiles measured by the Dual-frequency Precipitation Radar (DPR) on board the GPM Core Observatory, the first spaceborne radar operating at Ka and Ku band (see figure below) and provides vertical profiles of liquid and solid precipitation microphysics, and precipitation rate near the surface. Some details on the 2B-CMB algorithm can be found in <a href=\"https://gpm.nasa.gov/resources/documents/gpmdpr-level-2-algorithm-theoretical-basis-document-atbd/\">GMI/DPR Level 2 Algorithm Theoretical Basis Document (ATBD) </a>.\n",
    "\n",
    "The GMI TBs and the DPR rainfall rates (from 2B-CMB) have been matched to build the dataset using a nearest neighbour approach. Only pixels over ocean and sea where rainfall has been observed (2B-CMB rainfall rate > 0 mm/h) are selected to build the dataset (for a total of 61906 pixels).\n",
    "![title](https://rmets.onlinelibrary.wiley.com/cms/asset/6e4beac5-ee9c-4de1-a780-0c34bd06987b/qj3313-fig-0001-m.jpg)\n",
    "_from: Skofronick-Jackson et al. 2018 doi:10.1002/qj.3313_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5cf4f",
   "metadata": {},
   "source": [
    "### 3) Training and test data split\n",
    "The objective of Machine Learning is to create a model which is able to learn from training data and consequently make consistent predictions in the presence of unseen data. In order to achieve this, the objective is to minimize a loss function. This is done by identifying a vector of model weights which minimizes the loss function. \n",
    "<br>\n",
    "<br>\n",
    "In order to train a Machine Learning model which is able to make successful predictions and therefore generalize sufficiently well in the presence of indipendent data coming from the similar distribution, a standard practice is to divide the dataset into __training__ and __test sets__. Generally speaking, the Machine Learning best practice suggests to retain about 80% of the dataset for training and the rest 20% for test purpose. Another common practice is to divide the dataset into three sets, namely train, test and validation in order to tune the model hyperparameters (using the test) and then validate the model on an independent set (validation). \n",
    "<br>\n",
    "<br> In our case, considering that our training set is quite small, we decide to opt for a __50%__ split between training and test set, to allow for a model testing on a comparable size dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483220f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, label_dataset):\n",
    "    \n",
    "    choice=np.mod(range(0,len(tensor_df)),2)==0 #this variable is true for even positions in the obseravtions sequence \n",
    "    X_train = dataset[choice==0]\n",
    "    X_test = dataset[choice]\n",
    "    y_train = label_dataset[choice==0]\n",
    "    y_test = label_dataset[choice]   \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_dataset(tensor_df, label_df)\n",
    "\n",
    "# Scaling: Standardize features by removing the mean and scaling to unit variance.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) #mean and variance are calculated on the training dataset and applied to the training dataset\n",
    "X_test_scaled = scaler.transform(X_test)       #mean and variance (previously calculated) are applied to the test dataset\n",
    "\n",
    "\n",
    "print( len(tensor_df))\n",
    "print( X_train.shape)\n",
    "print( X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb7b22",
   "metadata": {},
   "source": [
    "Subsequently we define the __model hyperparameters__, which influence the model ability to learn.\n",
    "<br><br>\n",
    "- The __learning rate__ is the model speed of the learning process: a too high learning rate can save some training time at the expenses of letting the model converge (i.e., minimizing the loss function); a small learning rate can significantly increase the training time and still fail to achieve an efficient minimum. By default, NNs are trained with a decreasing linear schedule, allowing for a constant decrease in the learning rate which helps the model converge.\n",
    "<br>\n",
    "- The __batch size__ is the number of samples to include in a single training iteration. The total number of batches can be obtained from: size of the training data/batch_size\n",
    "<br>\n",
    "- The number of __epochs__ is the number times that the learning algorithm will work through the entire training dataset\n",
    "<br>\n",
    "- Finally, setting the __verbose__ option equal to 1 allow us to obtain information on the training while in progress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "learning_rate1= 0.001  #the learning rate is the step size at each iteration while moving toward a minimum of a loss function\n",
    "epochs = 1600          #An epoch in machine learning means one complete pass of the training dataset through the algorithm. \n",
    "batch_size = 8000      #the batch size is the number of training examples utilized in one iteration. A large batch size should make the training faster but may lead to memory saturation\n",
    "\n",
    "\n",
    "verbose = 1\n",
    "# Set the input shape\n",
    "input_shape = X_train.shape[1]\n",
    "print(f'Feature shape: {input_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d0957",
   "metadata": {},
   "source": [
    "### 4) Model training\n",
    "\n",
    "We create a simple model architecture given the small training dataset. In this case we construct a MLP model with two hidden layers. The first layer contains 10 perceptrons, while the second is made of 20 perceptrons. We use a sigmoid activation function (transfer function used in both hidden layers). Finally, we use the mean squared error (MSE) as loss function to be minimised and display the mean average error (MAE) for each training iteration (epoch).\n",
    "<br> By printing the model it is possible to observe the shape of each layer and the total number of weights to be updated with the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14df0fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training phase with the training dataset\n",
    "def train():\n",
    "    # here the network achitecture is defined: it is a feed forward neural network with 2 hidden layers, \n",
    "    # 20 perceptrons in the fisrt hidden layer and 10 in the second. Sigmoids are used as transfer function in both hidden layers.\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=input_shape, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(10, kernel_initializer='normal', activation='sigmoid')) ###second hidden\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='linear')) #output\n",
    "    model.summary()\n",
    "    #the optimizer is the algorithm used for the training. Adam is a standard choice, but Scale conjugate gradient (SGD), is also very efficient.\n",
    "    optimizer = optimizers.Adam(lr=learning_rate1)\n",
    "    #optimizer = optimizers.experimental.SGD( learning_rate=learning_rate1)\n",
    "    \n",
    "    # here the model optimzer and the loss function to be minimized during training (mean squared error, MSE) are defined\n",
    "    # the mean absolute error (mae) is also computed as additional metrics\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae']) \n",
    "    \n",
    "    # the training dataset, the batch size and the number of epochs to be used re defined\n",
    "    history = model.fit(\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "    # validation is also carried out\n",
    "    # monitoring loss and metrics on the test dataset\n",
    "    # at the end of each epoch\n",
    "        validation_data=(X_test_scaled, y_test),)\n",
    "    model.save('./mlp_model.h5') #the model is saved at the end of the training phase in an HFD5 output file\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b859c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, history = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ecbfb5",
   "metadata": {},
   "source": [
    "### 5) Model evaluation \n",
    "\n",
    "Here we plot the learning curves, i.e., the MAE obtained ad each iteration (epoch) as a function of the epoch number. By observing the learning curves, we clearly see that the training and test values of the loss function decrease with the number of epochs. However, the decreasing rate of the two curves slows down, and the two curves become almost flat somewhat near the 1400th epoch, which means that the training of the model is almost completed. Moreover, the two (train and test) loss curves are very similar to each other, this is a clear sign of the generalizability of the model. In other terms, if we train on a dataset and we get the same result on an indipendent test dataset (with different observations) we expect that the application of the model to any other dataset will still give the same accuracy (i.e., validation dataset which is not considered in this exercise).\n",
    "<br> Overall, it is possible to observe that the two curves do not fully reach and asymptotic value, and it may be possible that a further training of the model brigns to slightly better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot learning curves\n",
    "plt.title('Learning Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.plot((history.history['loss']), label='train')\n",
    "plt.plot((history.history['val_loss']), label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f1bc2",
   "metadata": {},
   "source": [
    "### 6) Further training of the model and overfitting\n",
    "\n",
    "The model can be further trained for even more Epochs, but it will be subject to overfitting. An overfitting model reproduces too closely the target variable in a specific training dataset, and may therefore fail to fit additional data or predict future observations reliably.  In other words, the model memorizes the training dataset too well, learning also the noise included, and it is not able to generalise any more. <br>The following code loads the model that has been trained in section 6 and trains it further for additional 1000 epochs, using the same training hyperparameters (learning rate, batch size, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b02736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training phase with the training dataset\n",
    "fine_tune_epochs = 1000\n",
    "total_epochs =  epochs + fine_tune_epochs\n",
    "def train_fine(model):\n",
    "\n",
    "    optimizer = optimizers.Adam(lr=learning_rate1)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "    history_fine = model.fit(\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=total_epochs,\n",
    "        initial_epoch=history.epoch[-1],\n",
    "    # We also carry out some validation to\n",
    "    # monitor loss and metrics on the test dataset\n",
    "    # at the end of each epoch\n",
    "        validation_data=(X_test_scaled, y_test),)\n",
    "        #validation_data=(X_test, np.log10(y_test)),)\n",
    "    model.save('./mlp_model_1.h5')\n",
    "    return model, history_fine\n",
    "new_model = tf.keras.models.load_model('./mlp_model.h5')\n",
    "model_fine, history_fine = train_fine(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8471c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "plt.title('Learning Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.plot((history_fine.history['loss']), label='train')\n",
    "plt.plot((history_fine.history['val_loss']), label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e69af",
   "metadata": {},
   "source": [
    "Overfitting can be observed as the training loss continues to decrease with the Epochs, improving the accuracy, while the test loss shows a slower decrease. With more training epochs the test loss should reach an asymptotic value and not decrease anymore. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70346717",
   "metadata": {},
   "source": [
    "### 7) Testing the trained model on the initial case study\n",
    "\n",
    "The trained model is now applied to a real case study, the GMI overpass of Medicane Ianos on 16/09/2020 seen before, and the reult will be compared with the surface precipitation estimates provided by the NASA Goddard Profiling Algorithm, the official GPM preciptiation product for the GMI radiometer. Some details on the GPROF algorithm can be found in <a href=\"https://gpm.nasa.gov/resources/documents/gpm-gprof-algorithm-theoretical-basis-document-atbd/\">GPM GPROF Algorithm Theoretical Basis Document (ATBD)  </a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c577096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap \n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "import h5py\n",
    "import warnings\n",
    "\n",
    "# defining the function to plot\n",
    "def plot_rr_map(RR,lat,lon,colorAxisMin,colorAxisMax):\n",
    "    # set the lat/lon borders of the map\n",
    "    latborders=[34,40]\n",
    "    lonborders=[14,22]\n",
    "    m = Basemap(projection='merc',\n",
    "        resolution='l',\n",
    "        lat_ts=20,\n",
    "        llcrnrlat=latborders[0],urcrnrlat=latborders[1],\\\n",
    "        llcrnrlon=lonborders[0],urcrnrlon=lonborders[1])\n",
    "\n",
    "    \n",
    "#E set min/max values of the colorbar\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        im1 = m.pcolor(lon[:],lat[:],RR[:],shading='nearest',cmap='turbo',latlon=True,vmin=colorAxisMin,vmax=colorAxisMax)\n",
    "    m.drawcoastlines()\n",
    "# set parallels and meridians\n",
    "\n",
    "    dparal=2 #separation in deg between drawn parallels\n",
    "    parallels = np.arange(latborders[0],latborders[1],dparal)\n",
    "    dmerid=2 #separation in deg between drawn meridians\n",
    "    meridians = np.arange(lonborders[0],lonborders[1],dmerid)\n",
    "    m.drawparallels(parallels,labels=[1,0,0,0],fontsize=15)\n",
    "    m.drawmeridians(meridians,labels=[0,0,0,1],fontsize=15) \n",
    "# add colorbar.\n",
    "    cbar = m.colorbar(location='right',pad=\"5%\")\n",
    "    cbar.set_label('Rainfall Rate [mm/h]')\n",
    "    \n",
    "gmi_file_path='./1C-R.GPM.GMI.XCAL2016-C.20200916-S130832-E144106.037225.V07A.HDF5'\n",
    "hf=h5py.File(gmi_file_path,'r')\n",
    "lat1=hf['S1/Latitude'][:]\n",
    "lon1=hf['S1/Longitude'][:]\n",
    "TB1=hf['S1/Tc'][:]\n",
    "TB2=hf['S2/Tc'][:]\n",
    "hf.close()\n",
    "\n",
    "TB=np.concatenate((TB1,TB2,),axis=2)\n",
    "TB=np.reshape(TB,(2963*221,13))\n",
    "TB[np.any(TB<=0,axis=1),:]=np.NaN\n",
    "TB = scaler.transform(TB)\n",
    "predictions = model.predict(TB)\n",
    "predictions = np.reshape(predictions,(2963,221))\n",
    "\n",
    "\n",
    "gprof_file_path='./2A.GPM.GMI.GPROF2021v1.20200916-S130832-E144106.037225.V07A.HDF5'\n",
    "hf=h5py.File(gprof_file_path,'r')\n",
    "lat2=hf['S1/Latitude'][:]\n",
    "lon2=hf['S1/Longitude'][:]\n",
    "gprof_rr=hf['S1/surfacePrecipitation'][:]\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf5179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(24,16))\n",
    "ax = fig.add_subplot(121)\n",
    "plot_rr_map(predictions,lat1,lon1,0,40)\n",
    "plt.title('Exercise1 model',fontsize=12)\n",
    "ax = fig.add_subplot(122)\n",
    "plot_rr_map(gprof_rr,lat2,lon2,0,40)\n",
    "plt.title('GPROF',fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d43ef5",
   "metadata": {},
   "source": [
    "The result obtained by our NN model seems quite acceptable, the rainfall patterns predicted by our model are quite similar to those of GPROF, which is based on a physically-based Bayesian approach and is a very complex algorithm. It is important to highlight that:\n",
    " 1. The GMI observation of Madicane Ianos is not included in the training dataset (training dataset is built from 10 orbits of March 2014, while Medicane Ianos occured on 16/09/2020).\n",
    " 2. The NN model precipitation rate estimates over land and coastal areas are not reliable, as the training dataset does not include measurements over land (the precipitation retrieval over land is much more complex than over ocean and it has not been address in this exercise).\n",
    " 3. The high rainfall rates are underestimated, probably due to the small size of the training dataset, since intense rainfall is rare and probably it is not well represented in our limited-size training dataset.\n",
    " 4. Very low precipitation rates are present also in areas where the GPROF algorithm does not predict any precipitation, since the training dataset includes only pixels with precipitation (rainfall rate > 0 mm/h), and it has not learned how to disciminate between rain and no-rain pixels (precipitation detection is often handled using a different ML approach).\n",
    " 5. Remember that the trainig dataset is very small (10 orbits over ocean only) and the NN model obtained with this exercise is not suitable for operational or scientific purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f91045",
   "metadata": {},
   "source": [
    "# Addtional Exercises\n",
    "\n",
    "Using the following code try to train the model differently. In particular:\n",
    "\n",
    "1. try to modify the learning rate and the epoch number (try learning rates 0.01, 0.002, 0.0005 0.0001)\n",
    "2. modify the model architecture, varying the number of perceptrons in each hidden layer (40/10, 20/30)\n",
    "3. try to remove one or more input GMI channels (e.g. the first two at 10 GHz which are more directly impacted by surface precipitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b159a3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import utils\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# model hyperparameters\n",
    "learning_rate1= 0.001  #the learning rate is the step size at each iteration while moving toward a minimum of a loss function\n",
    "epochs = 1600          #An epoch in machine learning means one complete pass of the training dataset through the algorithm. \n",
    "batch_size = 8000      #the batch size is the number of training examples utilized in one iteration. A large batch size should make the training faster but may lead to memory saturation\n",
    "num_percept_layer1=20  #number of perceptron 1st hidden layer\n",
    "num_percept_layer2=10  #number of perceptron 2nd hidden layer\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    data_path = './dataset2_GMI_DPR_RR.nc'\n",
    "    ds = xr.open_dataset(data_path)\n",
    "    train_df = ds['tb'].to_dataframe().unstack()\n",
    "    target = ds['rr'].to_dataframe()\n",
    "    tensor_df = tf.convert_to_tensor(train_df, dtype=np.float)\n",
    "    label_df = tf.convert_to_tensor(target, dtype=np.float)\n",
    "    return tensor_df,label_df\n",
    "\n",
    "def split_dataset(dataset, label_dataset):\n",
    "    #split = round(test_split* len(tensor_df))\n",
    "    choice=np.mod(range(0,len(tensor_df)),2)==0\n",
    "    X_train = dataset[choice==0]\n",
    "    X_test = dataset[choice]\n",
    "    y_train = label_dataset[choice==0]\n",
    "    y_test = label_dataset[choice]   \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_dataset(tensor_df, label_df)\n",
    "\n",
    "# Scaling: Standardize features by removing the mean and scaling to unit variance.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) #mean and variance are calculated on the training dataset and applied to the training dataset\n",
    "X_test_scaled = scaler.transform(X_test)       #mean and variance (previously calculated) are applied to the test dataset\n",
    "\n",
    "# Set the input shape\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "\n",
    "def train():\n",
    "    # here the network achitecture is defined: here it is a feed forward neural network with 2 hidden layers, \n",
    "    # Sigmoids are used as transfer function in both hidden layers.\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_percept_layer1, input_dim=input_shape, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(num_percept_layer2, kernel_initializer='normal', activation='sigmoid')) ###second hidden\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='linear')) #output\n",
    "    model.summary()\n",
    "    #the optimizer is the algorithm used for the training. Adam is a standard choice, Scale conjugate gradient (SGD), is also very efficient.\n",
    "    optimizer = optimizers.Adam(lr=learning_rate1)\n",
    "    #optimizer = optimizers.experimental.SGD( learning_rate=learning_rate1)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae']) # the loss function to be minimized during training is the mean squared error (MSE)\n",
    "    history = model.fit(\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "    # We also carry out some validation to\n",
    "    # monitor loss and metrics on the test dataset\n",
    "    # at the end of each epoch\n",
    "        validation_data=(X_test_scaled, y_test),)\n",
    "        #validation_data=(X_test, np.log10(y_test)),)\n",
    "    return model, history\n",
    "\n",
    "\n",
    "verbose = 1\n",
    "# training with the train dataset\n",
    "model, history = train()\n",
    "\n",
    "model.save('./exer2_model.h5')\n",
    "\n",
    "# plot learning curves\n",
    "plt.title('Learning Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.plot((history.history['loss']), label='train')\n",
    "plt.plot((history.history['val_loss']), label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
